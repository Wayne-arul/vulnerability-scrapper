import requests
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain import hub
from langchain_openai import OpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import os

# Set up API keys
SERP_API_KEY = ''

# Set up environment variables for API keys
os.environ[
    "OPENAI_API_KEY"] = ""
os.environ["LANGCHAIN_API_KEY"] = ""
LANGCHAIN_API_KEY = ""
LANGCHAIN_TRACING_V2 = "true"


# Function to search Google using SerpAPI
def search_security_advisories(company_name):
    query = f"{company_name} security advisory"
    params = {
        'q': query,
        'hl': 'en',
        'gl': 'us',
        'api_key': SERP_API_KEY
    }
    response = requests.get('https://serpapi.com/search.json', params=params)
    results = response.json().get('organic_results', [])

    # Return only the first link
    if results:
        return results[0]['link']
    else:
        print(f"No results found for {company_name}")
        return None


# Function to scrape content from URLs
def scrape_content(url):
    try:
        loader = WebBaseLoader(url)
        documents = loader.load()
        combined_content = " ".join([doc.page_content for doc in documents])
        return combined_content
    except Exception as e:
        print(f"Failed to scrape {url}: {e}")
        return ""


# RAG pipeline function using LangChain
def rag_pipeline(content):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_text(content)

    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_texts(texts, embeddings)

    prompt = hub.pull("rlm/rag-prompt")

    # Setting up QA chain
    qa_chain = (
            {
                "context": vectorstore.as_retriever() | RunnablePassthrough(),
                "question": RunnablePassthrough(),
            }
            | prompt
            | OpenAI()
            | StrOutputParser()
    )

    # Query input from user
    query = """
        List out all vulnerabilities published on September 18, 2024 and later.
        Provide details in the following format:
        * Product Name: <Product Name>
        * Product Version: <Product Version or NA if not available>
        * OEM name: <OEM Name>
        * Severity Level (Critical/High/Medium/Low): <Severity>
        * Vulnerability: <Detailed Description of Vulnerability>
        * Mitigation Strategy: <Mitigation or Patch Details, with URL if available>
        * Published Date: September 19, 2024
        * Unique ID: CVE-<CVE ID if available>
        
        
       
    """
#  If multiple vulnerabilities are present, provide them one by one in the above format.
    result = qa_chain.invoke(query)

    print("Answer:")
    print(result)


# Main function to process companies
def process_company(company_name):
    url = search_security_advisories(company_name)

    if url:
        print(f"Processing {url}")
        content = scrape_content(url)
        if not content:
            return

        print(f"Scraped content from {url}")

        # Send new content to RAG
        rag_pipeline(content)


# Example usage
if __name__ == "__main__":
    company_names = ["Google"]  # Add more company names
    for company in company_names:
        process_company(company)
